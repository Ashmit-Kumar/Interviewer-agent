import { useEffect, useState, useCallback, useRef } from 'react';
import {
  Room,
  RoomEvent,
  Track,
  RemoteTrack,
  RemoteTrackPublication,
  ConnectionState,
} from 'livekit-client';

interface UseLiveKitOptions {
  roomName: string;
  token: string;
  wsUrl: string;
  onConnected?: () => void;
  onDisconnected?: () => void;
  onTrackSubscribed?: (track: RemoteTrack) => void;
  onTranscriptReceived?: (transcript: { role: string; content: string; timestamp: number }) => void;
  onActionReceived?: (action: string) => void;
}

export function useLiveKit({
  roomName,
  token,
  wsUrl,
  onConnected,
  onDisconnected,
  onTrackSubscribed,
  onTranscriptReceived,
  onActionReceived,
}: UseLiveKitOptions) {
  const [isConnected, setIsConnected] = useState(false);
  const [isMuted, setIsMuted] = useState(false);
  const [isAISpeaking, setIsAISpeaking] = useState(false);
  const [isAIThinking, setIsAIThinking] = useState(false);
  const [audioContextRestricted, setAudioContextRestricted] = useState(false);
  
  const roomRef = useRef<Room | null>(null);
  const connectingRef = useRef(false);
  const agentAudioRef = useRef<HTMLAudioElement | null>(null);

  // 1. Persistent Audio Element with enhanced logging
  useEffect(() => {
    if (!agentAudioRef.current) {
      const el = document.createElement('audio');
      el.autoplay = true;
      el.id = "livekit-agent-audio";
      document.body.appendChild(el);
      agentAudioRef.current = el;
      console.log('ðŸ”Š [AUDIO_SETUP] Global audio element created');
    }
    return () => {
      agentAudioRef.current?.remove();
      agentAudioRef.current = null;
    };
  }, []);

  const isConfigReady = Boolean(token) && Boolean(wsUrl) && Boolean(roomName);

  useEffect(() => {
    if (!isConfigReady || connectingRef.current) return;

    console.log('ðŸš€ [CONNECTION_START] Validating params...', { roomName, wsUrl });

    if (!token) {
      alert("Voice Error: Authentication token is missing.");
      return;
    }

    if (!roomRef.current) {
      roomRef.current = new Room({
        adaptiveStream: true,
        dynacast: true,
      });
    }
    
    const room = roomRef.current;

    // --- EVENT HANDLERS ---

    room.on(RoomEvent.Connected, () => {
      console.log('âœ… [ROOM_CONNECTED] Successfully joined:', room.name);
      setIsConnected(true);
      onConnected?.();
    });

    room.on(RoomEvent.Disconnected, (reason) => {
      console.warn('âš ï¸ [ROOM_DISCONNECTED] Reason:', reason);
      setIsConnected(false);
      onDisconnected?.();
    });

    // CRITICAL: Handle Autoplay/AudioContext restrictions
    room.on(RoomEvent.AudioPlaybackStatusChanged, () => {
      console.log('ðŸ”‡ [AUDIO_STATUS] Can playback:', room.canPlaybackAudio);
      setAudioContextRestricted(!room.canPlaybackAudio);
    });

    room.on(RoomEvent.TrackSubscribed, (track, publication) => {
      console.log(`ðŸŽµ [TRACK_SUBSCRIBED] Kind: ${track.kind}, Name: ${publication.trackName}, SID: ${track.sid}`);
      
      if (track.kind === Track.Kind.Audio) {
        setIsAISpeaking(true);
        onTrackSubscribed?.(track);

        if (agentAudioRef.current) {
          track.attach(agentAudioRef.current);
          console.log('ðŸ”— [AUDIO_ATTACH] Track attached to DOM element');

          agentAudioRef.current.play().catch((err) => {
            console.error('ðŸš« [AUTOPLAY_BLOCKED] Browser prevented audio:', err);
            setAudioContextRestricted(true);
          });
        }
      }
    });

    room.on(RoomEvent.TrackUnpublished, (pub) => {
      if (pub.kind === Track.Kind.Audio) {
        console.log('â¹ï¸ [TRACK_STOPPED] AI finished speaking');
        setIsAISpeaking(false);
      }
    });

    room.on(RoomEvent.DataReceived, (payload, participant) => {
      // Raw logging to debug missing messages
      try {
        console.log('ðŸ“¥ [RAW_DATA_RECEIVED] From:', participant?.identity);
        const decoder = new TextDecoder();
        const decodedString = decoder.decode(payload);
        console.log('ðŸ“„ [DECODED_DATA]:', decodedString);

        const data = JSON.parse(decodedString);

        if (data.type === 'transcript' && data.content) {
          console.log(`âœ¨ [MATCHED_TRANSCRIPT] ${data.role}:`, data.content);
          onTranscriptReceived?.({
            role: data.role,
            content: data.content,
            timestamp: Date.now(),
          });
          return;
        }

        if (data.type === 'state' && data.state) {
          console.log('ðŸ§  [STATE_UPDATE]:', data.state);
          setIsAIThinking(data.state === 'thinking');
          if (data.state === 'speaking') setIsAISpeaking(true);
          if (data.state === 'listening') setIsAISpeaking(false);
          return;
        }
        if (data.type === 'action' && data.action) {
          console.log('ðŸ [ACTION_RECEIVED]:', data.action);
          onActionReceived?.(data.action);
          return;
        }
      } catch (e) {
        console.error('âŒ [DATA_PARSE_ERROR] Payload was not JSON or decode failed:', e);
      }
    });

    // --- EXECUTE CONNECTION ---
    connectingRef.current = true;
    room.connect(wsUrl, token)
      .then(async () => {
        console.log('ðŸŽ¤ [MIC_START] Requesting microphone access...');
        try {
          await room.localParticipant.setMicrophoneEnabled(true);
          console.log('âœ… [MIC_SUCCESS] Microphone published');
        } catch (e: any) {
          console.error('âŒ [MIC_ERROR]', e);
          alert(`Microphone Error: ${e.message}. Please allow mic access and refresh.`);
        }
      })
      .catch((err) => {
        connectingRef.current = false;
        console.error('âŒ [CONNECTION_FAILED]', err);
        alert(`Failed to connect to voice server: ${err.message}`);
      });

    return () => {
      if (room.state !== ConnectionState.Disconnected) {
        console.log('ðŸ”Œ [CLEANUP] Disconnecting room');
        room.disconnect();
      }
      connectingRef.current = false;
    };
  }, [token, wsUrl]); // Only reconnect if token or wsUrl actually changes

  // --- ACTIONS ---

  const startAudio = useCallback(async () => {
    if (!roomRef.current) return;
    console.log('ðŸ”Š [START_AUDIO] Attempting to resume AudioContext...');
    try {
      await roomRef.current.startAudio();
      setAudioContextRestricted(false);
      console.log('âœ… [AUDIO_RESUMED] Context is now running');
    } catch (e) {
      console.error('âŒ [AUDIO_RESUME_FAILED]', e);
    }
  }, []);

  const toggleMute = useCallback(async () => {
    if (!roomRef.current) return;
    const local = roomRef.current.localParticipant;
    const nextMute = !isMuted;
    console.log(`ðŸŽ™ï¸ [MUTE_TOGGLE] Setting mute to: ${nextMute}`);
    await local.setMicrophoneEnabled(!nextMute);
    setIsMuted(nextMute);
  }, [isMuted]);

  const disconnect = useCallback(() => {
    roomRef.current?.disconnect();
  }, []);

  return {
    isConnected,
    isMuted,
    isAISpeaking,
    isAIThinking,
    audioContextRestricted,
    startAudio,
    toggleMute,
    disconnect,
    room: roomRef.current,
  };
}
